{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution for \"Titanic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_validate\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC\n",
    "import optuna\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/train.csv')\n",
    "df_test = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())\n",
    "print('')\n",
    "print(df_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr(),annot=True,cmap='bwr',linewidths=0.2)\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(10,8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complete missing values in 'Embarked' with mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")\n",
    "imp.fit(np.array(df.loc[:,\"Embarked\"]).reshape(-1,1))\n",
    "df[\"Embarked\"] = imp.transform(np.array(df.loc[:,\"Embarked\"]).reshape(-1,1))\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complete missing values in 'Age'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing indexes by honorific title\n",
    "train_mr_index = df['Name'].str.contains(' Mr. ')\n",
    "train_miss_index = df['Name'].str.contains(' Miss. ')\n",
    "train_mrs_index = df['Name'].str.contains(' Mrs. ')\n",
    "train_master_index = df['Name'].str.contains(' Master. ')\n",
    "test_mr_index = df_test['Name'].str.contains(' Mr. ')\n",
    "test_miss_index = df_test['Name'].str.contains(' Miss. ')\n",
    "test_mrs_index = df_test['Name'].str.contains(' Mrs. ')\n",
    "test_master_index = df_test['Name'].str.contains(' Master. ')\n",
    "\n",
    "# Calculation of the average value for each honorific title\n",
    "train_mr = df[df['Name'].str.contains(' Mr. ')]\n",
    "train_miss = df[df['Name'].str.contains(' Miss. ')]\n",
    "train_mrs = df[df['Name'].str.contains(' Mrs. ')]\n",
    "train_master = df[df['Name'].str.contains(' Master. ')]\n",
    "test_mr = df_test[df_test['Name'].str.contains(' Mr. ')]\n",
    "test_miss = df_test[df_test['Name'].str.contains(' Miss. ')]\n",
    "test_mrs = df_test[df_test['Name'].str.contains(' Mrs. ')]\n",
    "test_master = df_test[df_test['Name'].str.contains(' Master. ')]\n",
    "\n",
    "train_mr_num = train_mr['Age'].dropna().mean()\n",
    "train_miss_num = train_miss['Age'].dropna().mean()\n",
    "train_mrs_num = train_mrs['Age'].dropna().mean()\n",
    "train_master_num = train_master['Age'].dropna().mean()\n",
    "train_all_num = df['Age'].dropna().median()\n",
    "\n",
    "test_mr_num = test_mr['Age'].dropna().mean()\n",
    "test_miss_num = test_miss['Age'].dropna().mean()\n",
    "test_mrs_num = test_mrs['Age'].dropna().mean()\n",
    "test_master_num = test_master['Age'].dropna().mean()\n",
    "test_all_num = df_test['Age'].dropna().median()\n",
    "\n",
    "print(\"Mean value of honorific title 'Mr' in train data = \" + str(train_mr_num))\n",
    "print(\"Mean value of honorific title 'Miss' in train data = \" + str(train_miss_num))\n",
    "print(\"Mean value of honorific title 'Mrs' in train data = \" + str(train_mrs_num))\n",
    "print(\"Mean value of honorific title 'Master' in train data = \" + str(train_master_num))\n",
    "print(\"Median of train data = \" + str(train_all_num), '\\n')\n",
    "\n",
    "print(\"Mean value of honorific title 'Mr' in test data = \" + str(test_mr_num))\n",
    "print(\"Mean value of honorific title 'Miss' in test data = \" + str(test_miss_num))\n",
    "print(\"Mean value of honorific title 'Mrs' in test data = \" + str(test_mrs_num))\n",
    "print(\"Mean value of honorific title 'Master' in test data = \" + str(test_master_num))\n",
    "print(\"Median of test data = \" + str(test_all_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Completion of the average value for each honorific title for the missing value \"Age\"\n",
    "df['Age'][train_mr_index] = train_mr['Age'].fillna(32)\n",
    "df['Age'][train_miss_index] = train_master['Age'].fillna(22)\n",
    "df['Age'][train_mrs_index] = train_mrs['Age'].fillna(36)\n",
    "df['Age'][train_master_index] = train_master['Age'].fillna(5)\n",
    "df['Age'] = df['Age'].fillna(28)\n",
    "\n",
    "df_test['Age'][test_mr_index] = test_mr['Age'].fillna(32)\n",
    "df_test['Age'][test_miss_index] = test_miss['Age'].fillna(22)\n",
    "df_test['Age'][test_mrs_index] = test_mrs['Age'].fillna(39)\n",
    "df_test['Age'][test_master_index] = test_master['Age'].fillna(7)\n",
    "df_test['Age'] = df_test['Age'].fillna(27)\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complete missing values in 'Fare'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Fare'] = df_test['Fare'].fillna(df_test['Fare'].mean())\n",
    "\n",
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complete missing values in 'Cabin'**  \n",
    "\n",
    "Analyze survival rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(df[\"Survived\"].value_counts(), labels=[\"not Survived(0)\", \"Survived(1)\"], startangle=90, counterclock=False, autopct='%1.1f%%',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cabin = df[[\"Cabin\", \"Survived\"]]\n",
    "cabin[\"Cabin_init\"] = cabin[\"Cabin\"].map(lambda x:str(x)[0])\n",
    "\n",
    "cabin[\"Survived\"].groupby(cabin[\"Cabin_init\"]).agg([\"mean\", \"count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing value (n) is a lower survival rate than others and overall. So, in this case, I set those with Cabin data as 1 and those without as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Cabin\"] = df[\"Cabin\"].map(lambda x: 1 if x == x else 0)\n",
    "df_test[\"Cabin\"] = df_test[\"Cabin\"].map(lambda x: 1 if x == x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())\n",
    "print('')\n",
    "print(df_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical Data Conversion**  \n",
    "sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace({'Sex': {'male': 0, 'female': 1}}, inplace=True)\n",
    "df_test.replace({'Sex': {'male': 0, 'female': 1}}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embarked = pd.concat([df['Embarked'], df_test['Embarked']])\n",
    "\n",
    "embarked_ohe = pd.get_dummies(embarked)\n",
    "\n",
    "embarked_ohe_train = embarked_ohe[:891]\n",
    "embarked_ohe_test = embarked_ohe[891:]\n",
    "\n",
    "df = pd.concat([df, embarked_ohe_train], axis=1)\n",
    "df_test = pd.concat([df_test, embarked_ohe_test], axis=1)\n",
    "\n",
    "df.drop('Embarked', axis=1, inplace=True)\n",
    "df_test.drop('Embarked', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['FamilySize'] = df['Parch'] + df['SibSp'] + 1\n",
    "# discretization FamilySize\n",
    "df['FamilySize_bin'] = 'big'\n",
    "df.loc[df['FamilySize']==1,'FamilySize_bin'] = 'alone'\n",
    "df.loc[(df['FamilySize']>=2),'FamilySize_bin'] = 'family'\n",
    "\n",
    "df_test['FamilySize'] = df_test['Parch'] + df_test['SibSp'] + 1\n",
    "# discretization FamilySize\n",
    "df_test['FamilySize_bin'] = 'big'\n",
    "df_test.loc[df_test['FamilySize']==1,'FamilySize_bin'] = 'alone'\n",
    "df_test.loc[(df_test['FamilySize']>=2),'FamilySize_bin'] = 'family'\n",
    "\n",
    "df.replace({'FamilySize_bin': {'alone': 0, 'family': 1}}, inplace=True)\n",
    "df_test.replace({'FamilySize_bin': {'alone': 0, 'family': 1}}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'TicketFreq'] = df.groupby(['Ticket'])['PassengerId'].transform('count')\n",
    "\n",
    "df_test.loc[:, 'TicketFreq'] = df_test.groupby(['Ticket'])['PassengerId'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract honorifics\n",
    "df['honorific'] = df['Name'].map(lambda x: x.split(', ')[1].split('. ')[0])\n",
    "# Respectful processing\n",
    "df['honorific'].replace(['Col','Dr', 'Rev'], 'Rare',inplace=True) #Integrate honorifics\n",
    "df['honorific'].replace('Mlle', 'Miss',inplace=True)\n",
    "df['honorific'].replace('Ms', 'Miss',inplace=True)\n",
    "\n",
    "# Extract honorifics\n",
    "df_test['honorific'] = df_test['Name'].map(lambda x: x.split(', ')[1].split('. ')[0])\n",
    "# Respectful processing\n",
    "df_test['honorific'].replace(['Col','Dr', 'Rev'], 'Rare',inplace=True) #Integrate honorifics\n",
    "df_test['honorific'].replace('Mlle', 'Miss',inplace=True)\n",
    "df_test['honorific'].replace('Ms', 'Miss',inplace=True)\n",
    "\n",
    "df = pd.get_dummies(df, drop_first=True, columns=['honorific'])\n",
    "df_test = pd.get_dummies(df_test, drop_first=True, columns=['honorific'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Name', axis=1, inplace=True)\n",
    "df_test.drop('Name', axis=1, inplace=True)\n",
    "\n",
    "df.drop('Ticket', axis=1, inplace=True)\n",
    "df_test.drop('Ticket', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((set(df.keys()) - set(df_test.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Survived'] = 0\n",
    "df_test['honorific_Don'] = 0\n",
    "df_test['honorific_Major'] = 0\n",
    "df_test['honorific_Jonkheer'] = 0\n",
    "df_test['honorific_Sir'] = 0\n",
    "df_test['honorific_Mme'] = 0\n",
    "df_test['honorific_the Countess'] = 0\n",
    "df_test['honorific_Lady'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building a Baseline Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'Survived'\n",
    "drop_col = ['PassengerId','Survived','Fare', 'Cabin', 'Age', 'Parch', 'FamilySize', 'SibSp']\n",
    "# Retain only the features necessary for training\n",
    "train_feature = df.drop(columns=drop_col)\n",
    "test_feature = df_test.drop(columns=drop_col)\n",
    "train_tagert = df[target_col]\n",
    "# Split train data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_feature, train_tagert, test_size=0.2, random_state=0, stratify=train_tagert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survive_rate = y_train.sum()/len(y_train)\n",
    "print(f'survive rate:{survive_rate}')\n",
    "print(f'base line accuracy: {1 - survive_rate}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=0)\n",
    "rfc.fit(X_train, y_train)\n",
    "print('='*20)\n",
    "print('RandomForestClassifier')\n",
    "print(f'accuracy of train set: {rfc.score(X_train, y_train)}')\n",
    "print(f'accuracy of test set: {rfc.score(X_test, y_test)}')\n",
    "\n",
    "xgb = XGBClassifier(random_state=0)\n",
    "xgb.fit(X_train, y_train)\n",
    "print('='*20)\n",
    "print('XGBClassifier')\n",
    "print(f'accuracy of train set: {xgb.score(X_train, y_train)}')\n",
    "print(f'accuracy of train set: {xgb.score(X_test, y_test)}')\n",
    "\n",
    "lgb = LGBMClassifier(random_state=0)\n",
    "lgb.fit(X_train, y_train)\n",
    "print('='*20)\n",
    "print('LGBMClassifier')\n",
    "print(f'accuracy of train set: {lgb.score(X_train, y_train)}')\n",
    "print(f'accuracy of train set: {lgb.score(X_test, y_test)}')\n",
    "\n",
    "lr = LogisticRegression(random_state=0)\n",
    "lr.fit(X_train, y_train)\n",
    "print('='*20)\n",
    "print('LogisticRegression')\n",
    "print(f'accuracy of train set: {lr.score(X_train, y_train)}')\n",
    "print(f'accuracy of train set: {lr.score(X_test, y_test)}')\n",
    "\n",
    "svc = SVC(random_state=0)\n",
    "svc.fit(X_train, y_train)\n",
    "print('='*20)\n",
    "print('SVC')\n",
    "print(f'accuracy of train set: {svc.score(X_train, y_train)}')\n",
    "print(f'accuracy of train set: {svc.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = 5\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    param_grid_rfc = {\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 15),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 5),\n",
    "        'min_samples_split': trial.suggest_int(\"min_samples_split\", 7, 15),\n",
    "        \"criterion\": trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"]),\n",
    "        'max_features': trial.suggest_int(\"max_features\", 3, 10),\n",
    "        \"random_state\": 0\n",
    "    }\n",
    "\n",
    "    model = RandomForestClassifier(**param_grid_rfc)\n",
    "    \n",
    "    # Evaluate the model with 5-Fold CV / Accuracy\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    scores = cross_validate(model, X=X_train, y=y_train, cv=kf)\n",
    "    # Minimize, so subtract score from 1.0\n",
    "    return scores['test_score'].mean()\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "print(study.best_params)\n",
    "print(study.best_value)\n",
    "rfc_best_param = study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    param_grid_xgb = {\n",
    "        'min_child_weight': trial.suggest_int(\"min_child_weight\", 1, 5),\n",
    "        'gamma': trial.suggest_discrete_uniform(\"gamma\", 0.1, 1.0, 0.1),\n",
    "        'subsample': trial.suggest_discrete_uniform(\"subsample\", 0.5, 1.0, 0.1),\n",
    "        'colsample_bytree': trial.suggest_discrete_uniform(\"colsample_bytree\", 0.5, 1.0, 0.1),\n",
    "        'max_depth': trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"random_state\": 0\n",
    "    }\n",
    "\n",
    "    model = XGBClassifier(**param_grid_xgb)\n",
    "    \n",
    "    # Evaluate the model with 5-Fold CV / Accuracy\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    scores = cross_validate(model, X=X_train, y=y_train, cv=kf)\n",
    "    # Minimize, so subtract score from 1.0\n",
    "    return scores['test_score'].mean()\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "print(study.best_params)\n",
    "print(study.best_value)\n",
    "xgb_best_param = study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    param_grid_lgb = {\n",
    "        'num_leaves': trial.suggest_int(\"num_leaves\", 3, 10),\n",
    "        'learning_rate': trial.suggest_loguniform(\"learning_rate\", 1e-8, 1.0),\n",
    "        'max_depth': trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"random_state\": 0\n",
    "    }\n",
    "\n",
    "    model = LGBMClassifier(**param_grid_lgb)\n",
    "    \n",
    "    # Evaluate the model with 5-Fold CV / Accuracy\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    scores = cross_validate(model, X=X_train, y=y_train, cv=kf)\n",
    "    # Minimize, so subtract score from 1.0\n",
    "    return scores['test_score'].mean()\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "print(study.best_params)\n",
    "print(study.best_value)\n",
    "lgb_best_param = study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    param_grid_lr = {\n",
    "        'C' : trial.suggest_int(\"C\", 1, 100),\n",
    "        \"random_state\": 0\n",
    "    }\n",
    "\n",
    "    model = LogisticRegression(**param_grid_lr)\n",
    "    \n",
    "    # Evaluate the model with 5-Fold CV / Accuracy\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    scores = cross_validate(model, X=X_train, y=y_train, cv=kf)\n",
    "    # Minimize, so subtract score from 1.0\n",
    "    return scores['test_score'].mean()\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "print(study.best_params)\n",
    "print(study.best_value)\n",
    "lr_best_param = study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    param_grid_svc = {\n",
    "        'C' : trial.suggest_int(\"C\", 50, 200),\n",
    "        'gamma': trial.suggest_loguniform(\"gamma\", 1e-4, 1.0),\n",
    "        \"random_state\": 0,\n",
    "        'kernel': 'rbf'\n",
    "    }\n",
    "\n",
    "    model = SVC(**param_grid_svc)\n",
    "    \n",
    "    # Evaluate the model with 5-Fold CV / Accuracy\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    scores = cross_validate(model, X=X_train, y=y_train, cv=kf)\n",
    "    # Minimize, so subtract score from 1.0\n",
    "    return scores['test_score'].mean()\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "print(study.best_params)\n",
    "print(study.best_value)\n",
    "svc_best_param = study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model with 5-Fold CV / Accuracy\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "rfc_best = RandomForestClassifier(**rfc_best_param)\n",
    "print('RandomForestClassifier')\n",
    "print('='*20)\n",
    "scores = cross_validate(rfc_best, X=train_feature, y=train_tagert, cv=kf)\n",
    "print(f'mean:{scores[\"test_score\"].mean()}, std:{scores[\"test_score\"].std()}')\n",
    "print('='*20)\n",
    "\n",
    "xgb_best = XGBClassifier(**xgb_best_param)\n",
    "print('XGBClassifier')\n",
    "print('='*20)\n",
    "scores = cross_validate(xgb_best, X=train_feature, y=train_tagert, cv=kf)\n",
    "print(f'mean:{scores[\"test_score\"].mean()}, std:{scores[\"test_score\"].std()}')\n",
    "print('='*20)\n",
    "\n",
    "lgb_best = LGBMClassifier(**lgb_best_param)\n",
    "print('LGBMClassifier')\n",
    "print('='*20)\n",
    "scores = cross_validate(lgb_best, X=train_feature, y=train_tagert, cv=kf)\n",
    "print(f'mean:{scores[\"test_score\"].mean()}, std:{scores[\"test_score\"].std()}')\n",
    "print('='*20)\n",
    "\n",
    "lr_best = LogisticRegression(**lr_best_param)\n",
    "print('LogisticRegression')\n",
    "print('='*20)\n",
    "scores = cross_validate(lr_best, X=train_feature, y=train_tagert, cv=kf)\n",
    "print(f'mean:{scores[\"test_score\"].mean()}, std:{scores[\"test_score\"].std()}')\n",
    "print('='*20)\n",
    "\n",
    "svc_best = SVC(**svc_best_param)\n",
    "print('SVC')\n",
    "print('='*20)\n",
    "scores = cross_validate(svc_best, X=train_feature, y=train_tagert, cv=kf)\n",
    "print(f'mean:{scores[\"test_score\"].mean()}, std:{scores[\"test_score\"].std()}')\n",
    "print('='*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Prepare classifiers for voting\n",
    "estimators = [\n",
    "    ('rfc', RandomForestClassifier(**rfc_best_param)),\n",
    "    ('xgb', XGBClassifier(**xgb_best_param)),\n",
    "    ('lgb', LGBMClassifier(**lgb_best_param)),\n",
    "    ('lr', LogisticRegression(**lr_best_param)),\n",
    "    ('svc', SVC(**lr_best_param))\n",
    "]\n",
    "voting = VotingClassifier(estimators)\n",
    "\n",
    "print('VotingClassifier')\n",
    "print('='*20)\n",
    "scores = cross_validate(voting, X=train_feature, y=train_tagert, cv=kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest\n",
    "rfc_best = RandomForestClassifier(**rfc_best_param)\n",
    "rfc_best.fit(train_feature, train_tagert)\n",
    "# XGBoost\n",
    "xgb_best = XGBClassifier(**xgb_best_param)\n",
    "xgb_best.fit(train_feature, train_tagert)\n",
    "# LightGBM\n",
    "lgb_best = LGBMClassifier(**lgb_best_param)\n",
    "lgb_best.fit(train_feature, train_tagert)\n",
    "# LogisticRegression\n",
    "lr_best = LogisticRegression(**lr_best_param)\n",
    "lr_best.fit(train_feature, train_tagert)\n",
    "# SVC\n",
    "svc_best = SVC(**svc_best_param)\n",
    "svc_best.fit(train_feature, train_tagert)\n",
    "# prediction\n",
    "pred = {\n",
    "    'rfc': rfc_best.predict(test_feature),\n",
    "    'xgb': xgb_best.predict(test_feature),\n",
    "    'lgb': lgb_best.predict(test_feature),\n",
    "    'lr': lr_best.predict(test_feature),\n",
    "    'svc': svc_best.predict(test_feature)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in pred.items():\n",
    "    pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(df_test.PassengerId, columns=['PassengerId']).reset_index(drop=True),\n",
    "            pd.DataFrame(value, columns=['Survived'])\n",
    "        ],\n",
    "        axis=1\n",
    "    ).to_csv(f'output_{key}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "1be749acb8b888617c40bd60977924bba0eae76a41ea427eea7ed14986f6c017"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
